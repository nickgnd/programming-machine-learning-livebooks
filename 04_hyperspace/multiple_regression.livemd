# Chapter 4: Hyperspace!

```elixir
Mix.install(
  [
    {:exla, "~> 0.5"},
    {:nx, "~> 0.5"},
    {:vega_lite, "~> 0.1.6"},
    {:kino, "~> 0.8.1"},
    {:kino_vega_lite, "~> 0.1.7"},
    {:explorer, "~> 0.5.6"},
    {:kino_explorer, "~> 0.1.4"}
  ],
  config: [nx: [default_backend: EXLA.Backend]]
)
```

## Upgrading the Learner

### Preparing Data

```elixir
data =
  __DIR__
  |> Path.join("pizza_3_vars.txt")
  |> Path.expand()
  |> File.read!()
  # convert any two or more spaces into a comma
  |> String.replace(~r/[[:blank:]]{2,}/, ",")
  |> Explorer.DataFrame.load_csv!()
```

```elixir
# Extract "Reservations", "Temperature", "Tourists" and "Pizzas"
# respectively as x1, x2, x3, y from the dataframe
x1 = Explorer.Series.to_list(data["Reservations"])
x2 = Explorer.Series.to_list(data["Temperature"])
x3 = Explorer.Series.to_list(data["Tourists"])
y = Explorer.Series.to_list(data["Pizzas"])
```

### Let's build the matrix x for input variables

```elixir
# Same of `numpy.column_stack((x1, x2, x3))` used in the book
x =
  [x1, x2, x3]
  |> Nx.tensor()
  |> Nx.transpose()
```

```elixir
# Inspect x shape
x.shape()
```

```elixir
# Get the first 2 rows of x
x[0..1]
```

### And reshape y into a matrix for labels

```elixir
# Same of `y.reshape(-1, 1)` used in the book
y = Nx.tensor([y]) |> Nx.transpose()
```

```elixir
# Inspect y shape
y.shape()
```

## Multiple Linear Regression

```elixir
defmodule C4.MultipleLinearRegression do
  import Nx.Defn

  @doc """
  Return the prediction tensor given the inputs and weight.
  """
  defn(predict(x, weight), do: Nx.dot(x, weight))

  @doc """
  Returns the mean squared error.
  """
  defn loss(x, y, weight) do
    predictions = predict(x, weight)
    errors = Nx.subtract(predictions, y)
    squared_error = Nx.pow(errors, 2)

    Nx.mean(squared_error)
  end

  @doc """
  Returns the derivative of the loss curve.
  """
  defn gradient(x, y, weight) do
    # in python:
    # 2 * np.matmul(X.T, (predict(X, w) - Y)) / X.shape[0]

    predictions = predict(x, weight)
    errors = Nx.subtract(predictions, y)
    n_examples = elem(Nx.shape(x), 0)

    Nx.transpose(x)
    |> Nx.dot(errors)
    |> Nx.multiply(2)
    |> Nx.divide(n_examples)
  end

  @doc """
  Computes the weight by training the system
  with the given inputs and labels, by iterating
  over the examples the specified number of times.
  """
  def train(x, y, iterations, lr) do
    Enum.reduce(0..(iterations - 1), init_weight(x), fn i, weight ->
      current_loss = loss(x, y, weight) |> Nx.to_number()
      IO.puts("Iteration #{i} => Loss: #{current_loss}")
      Nx.subtract(weight, Nx.multiply(gradient(x, y, weight), lr))
    end)
  end

  # Given n elements it returns a tensor
  # with this shape {n, 1}, each element
  # initialized to 0
  defnp init_weight(x) do
    Nx.broadcast(Nx.tensor([0]), {elem(Nx.shape(x), 1), 1})
  end
end
```

### Train the system

```elixir
iterations = Kino.Input.number("iterations", default: 10_000)
```

```elixir
lr = Kino.Input.number("lr (learning rate)", default: 0.001)
```

```elixir
iterations = Kino.Input.read(iterations)
lr = Kino.Input.read(lr)

weight = C4.MultipleLinearRegression.train(x, y, iterations, lr)
```

```elixir
loss = C4.MultipleLinearRegression.loss(x, y, weight) |> Nx.to_number()
```

## Bye bye, bias ðŸ‘‹

Quoting the book:

> The bias is just the **weight** of an input variable that happens to have the constant value 1.

Basically, this expression:

<!-- livebook:{"force_markdown":true} -->

```elixir
Å· = x1 * w1 + x2 * w2 + x3 * w3 + b
```

can be rewritten as:

<!-- livebook:{"force_markdown":true} -->

```elixir
Å· = x1 * w1 + x2 * w2 + x3 * w3 + x0 * b
```

where `x0` is a constant matrix of value 1 with `{30, 1}` shape.

```elixir
x0 = List.duplicate(1, length(x1))
```

And now let's add the new input `x0` to the `x` tensor.

```elixir
x =
  [x0, x1, x2, x3]
  |> Nx.tensor()
  |> Nx.transpose()
```

```elixir
weight = C4.MultipleLinearRegression.train(x, y, iterations, lr)
```

```elixir
loss = C4.MultipleLinearRegression.loss(x, y, weight)
```

A few predictions

```elixir
Enum.map(0..4, fn i ->
  prediction =
    x[i]
    |> C4.MultipleLinearRegression.predict(weight)
    |> Nx.squeeze()
    |> Nx.to_number()
    |> Float.round(4)

  label =
    y[i]
    |> Nx.squeeze()
    |> Nx.to_number()

  IO.inspect("x[#{i}] -> #{prediction} (label: #{label})")
end)

Kino.nothing()
```
